{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenster/Search/venv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# pip install pinecone tqdm\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import uuid\n",
    "\n",
    "from pinecone import Pinecone\n",
    "\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone, OAI, MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path='secrets.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Pinecone\n",
    "api_key = os.environ['PINECONE_API_KEY']\n",
    "environment = os.environ['PINECONE_ENVIRONMENT']\n",
    "pinecone = Pinecone(api_key=api_key, environment=environment)\n",
    "\n",
    "index_name = \"cosine-3072\"\n",
    "pinecone_index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding_3072(text):\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        input=[text]\n",
    "    ).data[0].embedding\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'OpenAI' object has no attribute 'admin'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenster/Search/venv/lib/python3.12/site-packages/pymongo/ocsp_support.py:280: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to this_update_utc.\n",
      "  if response.this_update > now:\n",
      "/Users/chenster/Search/venv/lib/python3.12/site-packages/pymongo/ocsp_support.py:284: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to next_update_utc.\n",
      "  if response.next_update and response.next_update < now:\n",
      "/Users/chenster/Search/venv/lib/python3.12/site-packages/pymongo/ocsp_cache.py:65: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to next_update_utc.\n",
      "  if value.next_update is None:\n",
      "/Users/chenster/Search/venv/lib/python3.12/site-packages/pymongo/ocsp_cache.py:71: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to this_update_utc.\n",
      "  value.this_update\n",
      "/Users/chenster/Search/venv/lib/python3.12/site-packages/pymongo/ocsp_cache.py:73: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to next_update_utc.\n",
      "  < value.next_update\n",
      "/Users/chenster/Search/venv/lib/python3.12/site-packages/pymongo/ocsp_cache.py:98: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to this_update_utc.\n",
      "  assert value.this_update is not None\n",
      "/Users/chenster/Search/venv/lib/python3.12/site-packages/pymongo/ocsp_cache.py:99: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to next_update_utc.\n",
      "  assert value.next_update is not None\n",
      "/Users/chenster/Search/venv/lib/python3.12/site-packages/pymongo/ocsp_cache.py:101: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to this_update_utc.\n",
      "  value.this_update\n",
      "/Users/chenster/Search/venv/lib/python3.12/site-packages/pymongo/ocsp_cache.py:103: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to next_update_utc.\n",
      "  < value.next_update\n",
      "/Users/chenster/Search/venv/lib/python3.12/site-packages/pymongo/ocsp_cache.py:81: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to next_update_utc.\n",
      "  cached_value.next_update is not None\n",
      "/Users/chenster/Search/venv/lib/python3.12/site-packages/pymongo/ocsp_cache.py:82: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to next_update_utc.\n",
      "  and cached_value.next_update < value.next_update\n"
     ]
    }
   ],
   "source": [
    "mongoUsername = urllib.parse.quote_plus(os.environ['MONGO_USR'])\n",
    "mongoPassword = urllib.parse.quote_plus(os.environ['MONGO_PWD'])\n",
    "uri = f\"mongodb+srv://{mongoUsername}:{mongoPassword}@cluster0.afizqne.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "# Create a new client and connect to the server\n",
    "mongoClient = MongoClient(uri, server_api=ServerApi('1'))\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_db = mongoClient['Search']\n",
    "namespace_collection = mongo_db[\"namespaces\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pinecone_namespaces_to_mongo(namespaces, index):\n",
    "    to_insert = []\n",
    "    for namespace in namespaces:\n",
    "        to_insert.append({\n",
    "            \"_id\": uuid.uuid4(),\n",
    "            \"name\": namespace,\n",
    "        })\n",
    "    namespace_collection.insert_many(namespaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_visible_filter(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a tuple (text : string, successful? : boolean)\n",
    "def url_to_text(url):\n",
    "    try:\n",
    "        page = urlopen(url)\n",
    "    except:\n",
    "        return \"\", False\n",
    "    html = page.read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.findAll(text=True)\n",
    "    visible = filter(non_visible_filter, text)\n",
    "    return \" \".join(item.strip() for item in visible if item.strip()), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_to_text(\"https://realpython.com/python-web-scraping-practical-introduction/#scrape-and-parse-text-from-websites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Reddit Comments\n",
    "\n",
    "Get the url with .json appended to return it as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = urlopen(\"https://www.reddit.com/r/learnpython/comments/16xvuu5/python_reddit_data_scraper_for_beginners/.json\")\n",
    "data = json.loads(page.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_from_json(data):\n",
    "    return data[0]['data']['children'][0]['data']['selftext']\n",
    "\n",
    "def get_comments_from_json(data):\n",
    "    allComments = []\n",
    "    comments_layer_1 = data[1]['data']['children']\n",
    "\n",
    "    bfs_queue = []\n",
    "    for i in range(len(comments_layer_1)):\n",
    "        bfs_queue.append([i])\n",
    "\n",
    "    while len(bfs_queue) > 0:\n",
    "        cur = bfs_queue.pop(0)\n",
    "        data = comments_layer_1\n",
    "\n",
    "        # iterate down to the target layer in the json tree/dict\n",
    "        for i in range(len(cur)):\n",
    "            if i == len(cur) - 1:\n",
    "                data = data[cur[i]]['data']\n",
    "            else:\n",
    "                data = data[cur[i]]['data']['replies']['data']['children']\n",
    "\n",
    "        allComments.append(data['body'])\n",
    "\n",
    "        # check for children/replies and add them to BFS queue\n",
    "        if data['replies'] != \"\":\n",
    "            for i in range(len(data['replies']['data']['children'])):\n",
    "                bfs_queue.append(cur + [i])\n",
    "    \n",
    "    return allComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Have you checked out PRAW? That\\'s the standard way to do this:\\n\\nhttps://praw.readthedocs.io/en/stable/\\n\\nAlternatively, you could look into PushshiftIO, which is a massive third-party scraper of Reddit data.\\n\\nhttps://pushshift.io/\\n\\nPRAW has everything but may cap what you can scrape. PushshiftIO doesn\\'t have everything, but it does have a lot, and IIRC there is no cap.\\n\\nLastly, the lowest tech but probably most labor intensive route is to just scrape directly off the site. This can be done by slapping \".json\" into the end of any URL to convert its entire contents into a JSON object, which you can then traverse and extract data from more easily than the HTML source. Like literally add \".json\" to the end of the URL at the top of your screen now and you\\'ll see what I mean.',\n",
       " \"As we say in France, we're in the same boat, mate!\",\n",
       " 'Hey! I am so excited to see your post here. I am also a linguistic student and now looking for a useful way to collect posts in Reddit. Have you found any solutions? Or do you have any suggestions? Thank you！',\n",
       " 'Same! :) would love to hear how you did it!',\n",
       " \"Thanks a  lot! I'll look into PushshiftIO\",\n",
       " '&gt;PRAW\\n\\nWhy do I get a error: externally-managed-environment when installing PRAW?',\n",
       " \"You're operating system/environment manages packages itself and pip is respecting it. Create a virtual environment and install it in that instead of globally.\"]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_comments_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperlink_comments_from_json(data):\n",
    "    allComments = []\n",
    "    links = []\n",
    "    comments_layer_1 = data[1]['data']['children']\n",
    "\n",
    "    bfs_queue = []\n",
    "    for i in range(len(comments_layer_1)):\n",
    "        bfs_queue.append([i])\n",
    "\n",
    "    while len(bfs_queue) > 0:\n",
    "        cur = bfs_queue.pop(0)\n",
    "        data = comments_layer_1\n",
    "\n",
    "        # iterate down to the target layer in the json tree/dict\n",
    "        for i in range(len(cur)):\n",
    "            if i == len(cur) - 1:\n",
    "                data = data[cur[i]]['data']\n",
    "            else:\n",
    "                data = data[cur[i]]['data']['replies']['data']['children']\n",
    "\n",
    "        comment = data['body']\n",
    "        html = data['body_html']\n",
    "        # look for first occurrence of href=\\\"\n",
    "        href_index = html.find('href=\\\"')\n",
    "        if href_index != -1:\n",
    "            href_index += 6 # only 6 because the \\ escape key is not included\n",
    "            href_end_index = html.find('\\\"', href_index)\n",
    "            link = html[href_index:href_end_index]\n",
    "            allComments.append(comment)\n",
    "            links.append(link)\n",
    "\n",
    "        # check for children/replies and add them to BFS queue\n",
    "        if data['replies'] != \"\":\n",
    "            for i in range(len(data['replies']['data']['children'])):\n",
    "                bfs_queue.append(cur + [i])\n",
    "    \n",
    "\n",
    "    return links, allComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['https://praw.readthedocs.io/en/stable/'],\n",
       " ['Have you checked out PRAW? That\\'s the standard way to do this:\\n\\nhttps://praw.readthedocs.io/en/stable/\\n\\nAlternatively, you could look into PushshiftIO, which is a massive third-party scraper of Reddit data.\\n\\nhttps://pushshift.io/\\n\\nPRAW has everything but may cap what you can scrape. PushshiftIO doesn\\'t have everything, but it does have a lot, and IIRC there is no cap.\\n\\nLastly, the lowest tech but probably most labor intensive route is to just scrape directly off the site. This can be done by slapping \".json\" into the end of any URL to convert its entire contents into a JSON object, which you can then traverse and extract data from more easily than the HTML source. Like literally add \".json\" to the end of the URL at the top of your screen now and you\\'ll see what I mean.'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hyperlink_comments_from_json(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating a List of Reddit threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
